{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Hyperparameters (Paper Table 3)\n",
    "class Config:\n",
    "    # Dataset parameters\n",
    "    dataset = 'cifar10'  # mnist, cifar10, cifar100\n",
    "    batch_size = 128\n",
    "    image_size = 32\n",
    "    \n",
    "    # Model architecture\n",
    "    embed_dim = 20\n",
    "    hidden_dim = 256\n",
    "    num_timesteps = 10\n",
    "    num_classes = 10  # Will be set automatically\n",
    "    \n",
    "    # Training parameters\n",
    "    epochs = 15\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-3\n",
    "    eta = 0.1\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Helper Functions\n",
    "def cosine_noise_schedule(t, T=config.num_timesteps):\n",
    "    \"\"\"Cosine noise schedule from paper\"\"\"\n",
    "    s = 0.008\n",
    "    f_t = torch.cos((t/T + s)/(1 + s) * torch.pi/2).clamp(min=1e-5)\n",
    "    alpha_bar = f_t / f_t[0]\n",
    "    return alpha_bar\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average for model stability\"\"\"\n",
    "    def __init__(self, beta=0.995):\n",
    "        self.beta = beta\n",
    "        self.step = 0\n",
    "\n",
    "    def update(self, model, ema_model):\n",
    "        with torch.no_grad():\n",
    "            for param, ema_param in zip(model.parameters(), ema_model.parameters()):\n",
    "                ema_param.data = self.beta * ema_param.data + (1 - self.beta) * param.data\n",
    "        self.step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fixed Model Architecture for Batched Timesteps\n",
    "class NoPropBlock(nn.Module):\n",
    "    \"\"\"Diffusion dynamics block with batched timestep handling\"\"\"\n",
    "    def __init__(self, embed_dim, hidden_dim, num_timesteps=10, image_channels=3):\n",
    "        super().__init__()\n",
    "        # Timestep embedding for multiple steps\n",
    "        self.t_embed = nn.Embedding(num_timesteps, embed_dim)\n",
    "        \n",
    "        # Image processing branch\n",
    "        self.image_embed = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*(config.image_size//4)**2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Noise processing branch\n",
    "        self.noise_embed = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Combined processing\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, x, t):\n",
    "        # t is a tensor of shape [batch_size]\n",
    "        t_emb = self.t_embed(t)  # Shape: [batch_size, embed_dim]\n",
    "        \n",
    "        # Add timestep embedding to noise\n",
    "        z = z + t_emb\n",
    "        \n",
    "        # Process inputs\n",
    "        x_embed = self.image_embed(x)  # Shape: [batch_size, hidden_dim]\n",
    "        z_embed = self.noise_embed(z)  # Shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([x_embed, z_embed], dim=1)\n",
    "        return self.combine(combined)\n",
    "\n",
    "class NoPropModel(nn.Module):\n",
    "    def __init__(self, num_timesteps=10):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.num_classes, config.embed_dim)\n",
    "        self.block = NoPropBlock(\n",
    "            config.embed_dim,\n",
    "            config.hidden_dim,\n",
    "            num_timesteps=num_timesteps\n",
    "        )\n",
    "        self.final_layer = nn.Linear(config.embed_dim, config.num_classes)\n",
    "        \n",
    "    def forward(self, z, x, t):\n",
    "        # t should be a tensor of timestep indices\n",
    "        return self.block(z, x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9904b3b2837542aa86c4600c9a362e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7513056496534aa7b1069dc00c8e03db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Fixed Dataset Loading with Hugging Face\n",
    "# Define collate function at top level\n",
    "def no_prop_collate_fn(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    return images, labels\n",
    "\n",
    "def get_dataset(config):\n",
    "    # Map dataset names to Hugging Face paths\n",
    "    dataset_map = {\n",
    "        'mnist': 'mnist',\n",
    "        'cifar10': 'cifar10',\n",
    "        'cifar100': 'cifar100'\n",
    "    }\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(dataset_map[config.dataset.lower()])\n",
    "    \n",
    "    # Set number of classes\n",
    "    if config.dataset.lower() == 'cifar100':\n",
    "        config.num_classes = 100\n",
    "    else:\n",
    "        config.num_classes = 10\n",
    "\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Function to apply transforms and convert to tensors\n",
    "    def apply_transforms(examples):\n",
    "        # Convert images to RGB (CIFAR) or grayscale (MNIST)\n",
    "        if config.dataset.lower() == 'mnist':\n",
    "            examples['image'] = [transform(image.convert('L')) for image in examples['img']]\n",
    "        else:\n",
    "            examples['image'] = [transform(image.convert('RGB')) for image in examples['img']]\n",
    "            \n",
    "        # Convert labels to tensors\n",
    "        examples['label'] = [torch.tensor(label) for label in examples['label']]\n",
    "        return examples\n",
    "\n",
    "    # Process datasets with proper tensor conversion\n",
    "    dataset = dataset.map(\n",
    "        apply_transforms,\n",
    "        batched=True,\n",
    "        batch_size=config.batch_size,\n",
    "        remove_columns=['img']  # Remove original image column\n",
    "    )\n",
    "\n",
    "    # Convert to PyTorch format\n",
    "    train_dataset = dataset['train'].with_format('torch', \n",
    "        columns=['image', 'label'],\n",
    "        output_all_columns=False\n",
    "    )\n",
    "    test_dataset = dataset['test'].with_format('torch', \n",
    "        columns=['image', 'label'],\n",
    "        output_all_columns=False\n",
    "    )\n",
    "\n",
    "    # Create DataLoaders with proper collation\n",
    "    def collate_fn(batch):\n",
    "        images = torch.stack([item['image'] for item in batch])\n",
    "        labels = torch.stack([item['label'] for item in batch])\n",
    "        return images, labels\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=no_prop_collate_fn,\n",
    "        pin_memory=False,  # Disable pin_memory for stability\n",
    "        num_workers=0,  # Disable multiprocessing\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=no_prop_collate_fn,\n",
    "        pin_memory=False,\n",
    "        num_workers=0,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Initialize datasets\n",
    "trainloader, testloader = get_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: nan: 100%|██████████| 391/391 [02:22<00:00,  2.74it/s]\n",
      "Epoch 2 Loss: nan: 100%|██████████| 391/391 [02:37<00:00,  2.49it/s]\n",
      "Epoch 3 Loss: nan: 100%|██████████| 391/391 [02:44<00:00,  2.38it/s]\n",
      "Epoch 4 Loss: nan: 100%|██████████| 391/391 [02:36<00:00,  2.50it/s]\n",
      "Epoch 5 Loss: nan: 100%|██████████| 391/391 [02:50<00:00,  2.29it/s]\n",
      "Epoch 6 Loss: nan: 100%|██████████| 391/391 [02:10<00:00,  3.00it/s]\n",
      "Epoch 7 Loss: nan: 100%|██████████| 391/391 [02:05<00:00,  3.13it/s]\n",
      "Epoch 8 Loss: nan: 100%|██████████| 391/391 [01:56<00:00,  3.36it/s]\n",
      "Epoch 9 Loss: nan: 100%|██████████| 391/391 [01:48<00:00,  3.60it/s]\n",
      "Epoch 10 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.51it/s]\n",
      "Epoch 11 Loss: nan: 100%|██████████| 391/391 [01:48<00:00,  3.61it/s]\n",
      "Epoch 12 Loss: nan: 100%|██████████| 391/391 [01:53<00:00,  3.45it/s]\n",
      "Epoch 13 Loss: nan: 100%|██████████| 391/391 [01:52<00:00,  3.47it/s]\n",
      "Epoch 14 Loss: nan: 100%|██████████| 391/391 [01:48<00:00,  3.60it/s]\n",
      "Epoch 15 Loss: nan: 100%|██████████| 391/391 [01:52<00:00,  3.48it/s]\n"
     ]
    }
   ],
   "source": [
    "#%% Training Loop (Algorithm 1)\n",
    "model = NoPropModel().to(config.device)\n",
    "ema_model = NoPropModel().to(config.device)\n",
    "ema_model.load_state_dict(model.state_dict())\n",
    "ema = EMA()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, \n",
    "                       weight_decay=config.weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize noise schedule\n",
    "timesteps = torch.arange(config.num_timesteps)\n",
    "alpha_bar = cosine_noise_schedule(timesteps)\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(trainloader)\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images = images.to(config.device)\n",
    "        labels = labels.to(config.device)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Convert labels to embeddings\n",
    "        u_y = model.embed(labels)\n",
    "        \n",
    "        # Sample random timestep\n",
    "        t = torch.randint(1, config.num_timesteps, (batch_size,)).to(config.device)\n",
    "        alpha_bar_t = alpha_bar[t].view(-1, 1).to(config.device)\n",
    "        \n",
    "        # Forward process (add noise)\n",
    "        epsilon = torch.randn_like(u_y)\n",
    "        z_t = torch.sqrt(alpha_bar_t) * u_y + torch.sqrt(1 - alpha_bar_t) * epsilon\n",
    "        \n",
    "        # Sample random timesteps as indices (shape [batch_size])\n",
    "        t = torch.randint(1, config.num_timesteps, (batch_size,)).to(config.device)\n",
    "\n",
    "        # Forward pass with batched timesteps\n",
    "        u_pred = model(z_t, images, t-1)  # t-1 for 0-based indexing\n",
    "        \n",
    "        # Compute loss (Equation 8)\n",
    "        snr_t = alpha_bar_t / (1 - alpha_bar_t)\n",
    "        snr_t_prev = alpha_bar[t-1].view(-1, 1) / (1 - alpha_bar[t-1].view(-1, 1))\n",
    "        loss_denoise = torch.mean((snr_t - snr_t_prev) * torch.norm(u_pred - u_y, dim=1)**2)\n",
    "        \n",
    "        # Final classification loss\n",
    "        logits = model.final_layer(z_t)\n",
    "        loss_cls = criterion(logits, labels)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss_cls + config.eta * loss_denoise\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.update(model, ema_model)\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/79 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m---> 32\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mema_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#%% Continuous-Time Variant (Algorithm 2)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 17\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, testloader)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Iterative denoising (Algorithm 1 inference)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_timesteps)):\n\u001b[1;32m---> 17\u001b[0m     u_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     alpha_bar_t \u001b[38;5;241m=\u001b[39m alpha_bar[t]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Update z using denoised prediction\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 63\u001b[0m, in \u001b[0;36mNoPropModel.forward\u001b[1;34m(self, z, x, t)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, x, t):\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# t should be a tensor of timestep indices\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[23], line 37\u001b[0m, in \u001b[0;36mNoPropBlock.forward\u001b[1;34m(self, z, x, t)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, x, t):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# t is a tensor of shape [batch_size]\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: [batch_size, embed_dim]\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# Add timestep embedding to noise\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     z \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m+\u001b[39m t_emb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:2233\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2227\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2228\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2229\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "#%% Evaluation\n",
    "@torch.no_grad()\n",
    "def evaluate(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(testloader):\n",
    "        images = images.to(config.device)\n",
    "        labels = labels.to(config.device)\n",
    "        \n",
    "        # Inference process\n",
    "        z = torch.randn(len(images), config.embed_dim).to(config.device)\n",
    "        \n",
    "        # Iterative denoising (Algorithm 1 inference)\n",
    "        for t in reversed(range(config.num_timesteps)):\n",
    "            u_pred = model(z, images, t)\n",
    "            alpha_bar_t = alpha_bar[t].item()\n",
    "            \n",
    "            # Update z using denoised prediction\n",
    "            z = (z - (1 - alpha_bar_t) * u_pred) / torch.sqrt(alpha_bar_t)\n",
    "            z += torch.sqrt(1 - alpha_bar_t) * torch.randn_like(z)\n",
    "        \n",
    "        # Final prediction\n",
    "        logits = model.final_layer(z)\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "accuracy = evaluate(ema_model, testloader)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "#%% Continuous-Time Variant (Algorithm 2)\n",
    "class NoPropCT(nn.Module):\n",
    "    \"\"\"Continuous-time NoProp with neural ODE\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.num_classes, config.embed_dim)\n",
    "        self.block = NoPropBlock(config.embed_dim, config.hidden_dim)\n",
    "        self.time_embed = nn.Embedding(100, config.embed_dim)\n",
    "        self.final_layer = nn.Linear(config.embed_dim, config.num_classes)\n",
    "        \n",
    "    def forward(self, z, x, t):\n",
    "        t_embed = self.time_embed(t)\n",
    "        return self.block(z + t_embed, x)\n",
    "\n",
    "# Training loop for continuous-time would involve:\n",
    "# - Random time sampling\n",
    "# - Neural ODE solver integration\n",
    "# - Different noise schedule handling\n",
    "# (Implementation similar to discrete-time but with continuous components)\n",
    "\n",
    "#%% Flow Matching Variant (Algorithm 3)\n",
    "class NoPropFM(nn.Module):\n",
    "    \"\"\"Flow matching variant\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.num_classes, config.embed_dim)\n",
    "        self.block = NoPropBlock(config.embed_dim, config.hidden_dim)\n",
    "        self.time_embed = nn.Embedding(100, config.embed_dim)\n",
    "        self.final_layer = nn.Linear(config.embed_dim, config.num_classes)\n",
    "        \n",
    "    def forward(self, z, x, t):\n",
    "        t_embed = self.time_embed(t)\n",
    "        return self.block(z + t_embed, x)\n",
    "\n",
    "# Training loop for flow matching would involve:\n",
    "# - Linear interpolation between noise and target\n",
    "# - Vector field prediction\n",
    "# - Anchor loss implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Continuous-Time Variant (Algorithm 2)\n",
    "class NoPropCT(nn.Module):\n",
    "    \"\"\"Continuous-time NoProp with neural ODE\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.num_classes, config.embed_dim)\n",
    "        self.block = NoPropBlock(config.embed_dim, config.hidden_dim)\n",
    "        self.time_embed = nn.Embedding(100, config.embed_dim)\n",
    "        self.final_layer = nn.Linear(config.embed_dim, config.num_classes)\n",
    "        \n",
    "    def forward(self, z, x, t):\n",
    "        t_embed = self.time_embed(t)\n",
    "        return self.block(z + t_embed, x)\n",
    "\n",
    "# Training loop for continuous-time would involve:\n",
    "# - Random time sampling\n",
    "# - Neural ODE solver integration\n",
    "# - Different noise schedule handling\n",
    "# (Implementation similar to discrete-time but with continuous components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Flow Matching Variant (Algorithm 3)\n",
    "class NoPropFM(nn.Module):\n",
    "    \"\"\"Flow matching variant\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.num_classes, config.embed_dim)\n",
    "        self.block = NoPropBlock(config.embed_dim, config.hidden_dim)\n",
    "        self.time_embed = nn.Embedding(100, config.embed_dim)\n",
    "        self.final_layer = nn.Linear(config.embed_dim, config.num_classes)\n",
    "        \n",
    "    def forward(self, z, x, t):\n",
    "        t_embed = self.time_embed(t)\n",
    "        return self.block(z + t_embed, x)\n",
    "\n",
    "# Training loop for flow matching would involve:\n",
    "# - Linear interpolation between noise and target\n",
    "# - Vector field prediction\n",
    "# - Anchor loss implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
