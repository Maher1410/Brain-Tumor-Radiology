{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 1. Import Dependencies --------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from skimage.filters import frangi, hessian\n",
    "from skimage.morphology import erosion, dilation, disk\n",
    "from skimage.feature import hessian_matrix_eigvals\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "#%% -------- 0. Configuration & Paths --------\n",
    "DATA_ROOT = Path(\"Radiographs\")\n",
    "SPLITS = {\n",
    "    'train': DATA_ROOT / \"train\",\n",
    "    'val': DATA_ROOT / \"val\",\n",
    "    'test': DATA_ROOT / \"test\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 2. Custom Preprocessing Pipeline --------\n",
    "class DentalPreprocessor:\n",
    "    def __init__(self, clip_limit=2.0, grid_size=(8,8)):\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        # Convert to numpy array\n",
    "        img_np = np.array(img)\n",
    "        \n",
    "        # CLAHE for contrast enhancement\n",
    "        clahe_img = self.clahe.apply(img_np)\n",
    "        \n",
    "        # Frangi vesselness filter\n",
    "        frangi_img = frangi(clahe_img, sigmas=range(1, 3, 1))\n",
    "        \n",
    "        # Entropy-driven dynamic morphology\n",
    "        entropy_img = self.calculate_entropy(clahe_img)\n",
    "        processed_img = self.dynamic_morphology(frangi_img, entropy_img)\n",
    "        \n",
    "        return torch.from_numpy(processed_img).float().unsqueeze(0)\n",
    "    \n",
    "    def calculate_entropy(self, img, kernel_size=7):\n",
    "        from skimage.filters.rank import entropy\n",
    "        return entropy(img, disk(kernel_size))\n",
    "    \n",
    "    def dynamic_morphology(self, img, entropy_img, threshold=0.5):\n",
    "        selem = disk(2)\n",
    "        normalized_entropy = (entropy_img - entropy_img.min()) / (entropy_img.max() - entropy_img.min())\n",
    "        mask = normalized_entropy > threshold\n",
    "        processed = np.zeros_like(img)\n",
    "        processed[mask] = dilation(img[mask], selem)\n",
    "        processed[~mask] = erosion(img[~mask], selem)\n",
    "        return \n",
    "    \n",
    "    #%% -------- 2.5 Custom Dataset Class --------\n",
    "class DentalDataset(Dataset):\n",
    "    def __init__(self, transform=None, mode='train'):\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Get the appropriate split directory\n",
    "        split_dir = SPLITS[mode]\n",
    "        \n",
    "        # Load male samples (class 0)\n",
    "        male_dir = split_dir / \"male\"\n",
    "        male_images = list(male_dir.glob(\"*.png\")) + list(male_dir.glob(\"*.jpg\"))\n",
    "        self.image_paths.extend(male_images)\n",
    "        self.labels.extend([0] * len(male_images))\n",
    "        \n",
    "        # Load female samples (class 1)\n",
    "        female_dir = split_dir / \"female\"\n",
    "        female_images = list(female_dir.glob(\"*.png\")) + list(female_dir.glob(\"*.jpg\"))\n",
    "        self.image_paths.extend(female_images)\n",
    "        self.labels.extend([1] * len(female_images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Call after dataset creation\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m \u001b[43mplot_class_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m show_sample_images(train_dataset)\n",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m, in \u001b[0;36mplot_class_distribution\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_class_distribution\u001b[39m():\n\u001b[1;32m----> 5\u001b[0m     counts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mbincount(\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mtargets)\n\u001b[0;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      7\u001b[0m     plt\u001b[38;5;241m.\u001b[39mbar(class_names, counts)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "#%% -------- 3. Data Visualization Cell --------\n",
    "class_names = ['Male', 'Female']  # Updated to male/female\n",
    "\n",
    "def plot_class_distribution():\n",
    "    counts = np.bincount(train_dataset.targets)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(class_names, counts)\n",
    "    plt.title('Class Distribution in Training Set')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def show_sample_images(dataset, num_images=6):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i in range(num_images):\n",
    "        idx = np.random.randint(len(dataset))\n",
    "        img, label = dataset[idx]\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.title(f\"Class: {class_names[label]}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call after dataset creation\n",
    "plot_class_distribution()\n",
    "show_sample_images(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 4. Hybrid CNN-Transformer Model --------\n",
    "class DentalGenderClassifier(nn.Module):\n",
    "    def __init__(self, patch_size=7, embed_dim=256, num_heads=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN Backbone\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((14, 14)))\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        self.patch_embed = nn.Linear(128*patch_size**2, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(embed_dim, 1)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # CNN feature extraction\n",
    "        features = self.cnn(x)\n",
    "        b, c, h, w = features.shape\n",
    "        \n",
    "        # Convert to patches\n",
    "        patches = features.unfold(2, 7, 7).unfold(3, 7, 7)\n",
    "        patches = patches.contiguous().view(b, -1, 128*7*7)\n",
    "        \n",
    "        # Transformer processing\n",
    "        embeddings = self.patch_embed(patches)\n",
    "        encoded = self.transformer(embeddings)\n",
    "        \n",
    "        # Classification\n",
    "        pooled = encoded.mean(dim=1)\n",
    "        return torch.sigmoid(self.classifier(pooled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 5. Training Configuration --------\n",
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    DentalPreprocessor(),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    DentalPreprocessor(),\n",
    "])\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = DentalDataset(transform=train_transform, mode='train')\n",
    "val_dataset = DentalDataset(transform=val_transform, mode='val')\n",
    "test_dataset = DentalDataset(transform=val_transform, mode='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = DentalGenderClassifier().to(model.device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 6. Training Loop --------\n",
    "best_accuracy = 0\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "        images = images.to(model.device)\n",
    "        labels = labels.float().to(model.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(model.device)\n",
    "            labels = labels.to(model.device)\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.squeeze() == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}: Train Loss {train_loss/len(train_loader):.4f}, Val Acc {accuracy:.2f}%\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 7. Model Evaluation --------\n",
    "test_dataset = DentalDataset(transform=val_transform, mode='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(model.device)\n",
    "        outputs = model(images)\n",
    "        preds = (outputs > 0.5).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 8. Model Interpretation --------\n",
    "def visualize_attention(model, img_tensor):\n",
    "    # Implementation would use hooks to extract attention maps\n",
    "    pass  # Add attention visualization implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 9. Inference Visualization Cell --------\n",
    "def predict_and_visualize(model, dataset, num_images=9):\n",
    "    model.eval()\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        idx = np.random.randint(len(dataset))\n",
    "        img, true_label = dataset[idx]\n",
    "        img_tensor = img.unsqueeze(0).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prob = model(img_tensor).item()\n",
    "            pred_label = int(prob > 0.5)\n",
    "        \n",
    "        plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(img.squeeze(), cmap='gray')\n",
    "        plt.title(f\"True: {class_names[true_label]}\\nPred: {class_names[pred_label]}\\nConf: {prob:.2f}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "predict_and_visualize(model, test_dataset)\n",
    "\n",
    "print(\"Full pipeline execution completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
