{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Hyperparameters (Paper Table 3)\n",
    "class Config:\n",
    "    # Dataset parameters\n",
    "    dataset = 'cifar10'  # mnist, cifar10, cifar100\n",
    "    batch_size = 128\n",
    "    image_size = 32\n",
    "    \n",
    "    # Model architecture\n",
    "    embed_dim = 20\n",
    "    hidden_dim = 256\n",
    "    num_timesteps = 10\n",
    "    num_classes = 10  # Will be set automatically\n",
    "    \n",
    "    # Training parameters\n",
    "    epochs = 150\n",
    "    learning_rate = 1e-3\n",
    "    weight_decay = 1e-3\n",
    "    eta = 0.1\n",
    "    \n",
    "    # Device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Helper Functions\n",
    "def cosine_noise_schedule(t, T=config.num_timesteps):\n",
    "    \"\"\"Cosine noise schedule from paper\"\"\"\n",
    "    s = 0.008\n",
    "    f_t = torch.cos((t/T + s)/(1 + s) * torch.pi/2).clamp(min=1e-5)\n",
    "    alpha_bar = f_t / f_t[0]\n",
    "    return alpha_bar\n",
    "\n",
    "class EMA:\n",
    "    \"\"\"Exponential Moving Average for model stability\"\"\"\n",
    "    def __init__(self, beta=0.995):\n",
    "        self.beta = beta\n",
    "        self.step = 0\n",
    "\n",
    "    def update(self, model, ema_model):\n",
    "        with torch.no_grad():\n",
    "            for param, ema_param in zip(model.parameters(), ema_model.parameters()):\n",
    "                ema_param.data = self.beta * ema_param.data + (1 - self.beta) * param.data\n",
    "        self.step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fixed Model Architecture for Batched Timesteps\n",
    "class NoPropBlock(nn.Module):\n",
    "    \"\"\"Diffusion dynamics block with batched timestep handling\"\"\"\n",
    "    def __init__(self, embed_dim, hidden_dim, num_timesteps=10, image_channels=3):\n",
    "        super().__init__()\n",
    "        # Timestep embedding for multiple steps\n",
    "        self.t_embed = nn.Embedding(num_timesteps, embed_dim)\n",
    "        \n",
    "        # Image processing branch\n",
    "        self.image_embed = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 16, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*(config.image_size//4)**2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Noise processing branch\n",
    "        self.noise_embed = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Combined processing\n",
    "        self.combine = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z, x, t):\n",
    "        # t is a tensor of shape [batch_size]\n",
    "        t_emb = self.t_embed(t)  # Shape: [batch_size, embed_dim]\n",
    "        \n",
    "        # Add timestep embedding to noise\n",
    "        z = z + t_emb\n",
    "        \n",
    "        # Process inputs\n",
    "        x_embed = self.image_embed(x)  # Shape: [batch_size, hidden_dim]\n",
    "        z_embed = self.noise_embed(z)  # Shape: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Combine features\n",
    "        combined = torch.cat([x_embed, z_embed], dim=1)\n",
    "        return self.combine(combined)\n",
    "\n",
    "class NoPropModel(nn.Module):\n",
    "    def __init__(self, num_timesteps=10):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.num_classes, config.embed_dim)\n",
    "        self.block = NoPropBlock(\n",
    "            config.embed_dim,\n",
    "            config.hidden_dim,\n",
    "            num_timesteps=num_timesteps\n",
    "        )\n",
    "        self.final_layer = nn.Linear(config.embed_dim, config.num_classes)\n",
    "        \n",
    "    def forward(self, z, x, t):\n",
    "        # t should be a tensor of timestep indices\n",
    "        return self.block(z, x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fixed Dataset Loading with Hugging Face\n",
    "# Define collate function at top level\n",
    "def no_prop_collate_fn(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    return images, labels\n",
    "\n",
    "def get_dataset(config):\n",
    "    # Map dataset names to Hugging Face paths\n",
    "    dataset_map = {\n",
    "        'mnist': 'mnist',\n",
    "        'cifar10': 'cifar10',\n",
    "        'cifar100': 'cifar100'\n",
    "    }\n",
    "    \n",
    "    # Load dataset from Hugging Face\n",
    "    dataset = load_dataset(dataset_map[config.dataset.lower()])\n",
    "    \n",
    "    # Set number of classes\n",
    "    if config.dataset.lower() == 'cifar100':\n",
    "        config.num_classes = 100\n",
    "    else:\n",
    "        config.num_classes = 10\n",
    "\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Function to apply transforms and convert to tensors\n",
    "    def apply_transforms(examples):\n",
    "        # Convert images to RGB (CIFAR) or grayscale (MNIST)\n",
    "        if config.dataset.lower() == 'mnist':\n",
    "            examples['image'] = [transform(image.convert('L')) for image in examples['img']]\n",
    "        else:\n",
    "            examples['image'] = [transform(image.convert('RGB')) for image in examples['img']]\n",
    "            \n",
    "        # Convert labels to tensors\n",
    "        examples['label'] = [torch.tensor(label) for label in examples['label']]\n",
    "        return examples\n",
    "\n",
    "    # Process datasets with proper tensor conversion\n",
    "    dataset = dataset.map(\n",
    "        apply_transforms,\n",
    "        batched=True,\n",
    "        batch_size=config.batch_size,\n",
    "        remove_columns=['img']  # Remove original image column\n",
    "    )\n",
    "\n",
    "    # Convert to PyTorch format\n",
    "    train_dataset = dataset['train'].with_format('torch', \n",
    "        columns=['image', 'label'],\n",
    "        output_all_columns=False\n",
    "    )\n",
    "    test_dataset = dataset['test'].with_format('torch', \n",
    "        columns=['image', 'label'],\n",
    "        output_all_columns=False\n",
    "    )\n",
    "\n",
    "    # Create DataLoaders with proper collation\n",
    "    def collate_fn(batch):\n",
    "        images = torch.stack([item['image'] for item in batch])\n",
    "        labels = torch.stack([item['label'] for item in batch])\n",
    "        return images, labels\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=no_prop_collate_fn,\n",
    "        pin_memory=False,  # Disable pin_memory for stability\n",
    "        num_workers=0,  # Disable multiprocessing\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=no_prop_collate_fn,\n",
    "        pin_memory=False,\n",
    "        num_workers=0,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Initialize datasets\n",
    "trainloader, testloader = get_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: nan: 100%|██████████| 391/391 [02:00<00:00,  3.24it/s]\n",
      "Epoch 2 Loss: nan: 100%|██████████| 391/391 [01:53<00:00,  3.44it/s]\n",
      "Epoch 3 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.56it/s]\n",
      "Epoch 4 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.53it/s]\n",
      "Epoch 5 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.57it/s]\n",
      "Epoch 6 Loss: nan: 100%|██████████| 391/391 [01:48<00:00,  3.61it/s]\n",
      "Epoch 7 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.52it/s]\n",
      "Epoch 8 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.53it/s]\n",
      "Epoch 9 Loss: nan: 100%|██████████| 391/391 [01:53<00:00,  3.44it/s]\n",
      "Epoch 10 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.51it/s]\n",
      "Epoch 11 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.43it/s]\n",
      "Epoch 12 Loss: nan: 100%|██████████| 391/391 [01:52<00:00,  3.49it/s]\n",
      "Epoch 13 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.52it/s]\n",
      "Epoch 14 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.41it/s]\n",
      "Epoch 15 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.41it/s]\n",
      "Epoch 16 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.42it/s]\n",
      "Epoch 17 Loss: nan: 100%|██████████| 391/391 [01:52<00:00,  3.49it/s]\n",
      "Epoch 18 Loss: nan: 100%|██████████| 391/391 [01:52<00:00,  3.49it/s]\n",
      "Epoch 19 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.52it/s]\n",
      "Epoch 20 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.50it/s]\n",
      "Epoch 21 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.52it/s]\n",
      "Epoch 22 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.53it/s]\n",
      "Epoch 23 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.52it/s]\n",
      "Epoch 24 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.49it/s]\n",
      "Epoch 25 Loss: nan: 100%|██████████| 391/391 [01:47<00:00,  3.64it/s]\n",
      "Epoch 26 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.55it/s]\n",
      "Epoch 27 Loss: nan: 100%|██████████| 391/391 [01:48<00:00,  3.62it/s]\n",
      "Epoch 28 Loss: nan: 100%|██████████| 391/391 [01:46<00:00,  3.67it/s]\n",
      "Epoch 29 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.56it/s]\n",
      "Epoch 30 Loss: nan: 100%|██████████| 391/391 [01:46<00:00,  3.67it/s]\n",
      "Epoch 31 Loss: nan: 100%|██████████| 391/391 [01:48<00:00,  3.60it/s]\n",
      "Epoch 32 Loss: nan: 100%|██████████| 391/391 [01:47<00:00,  3.62it/s]\n",
      "Epoch 33 Loss: nan: 100%|██████████| 391/391 [01:48<00:00,  3.60it/s]\n",
      "Epoch 34 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.58it/s]\n",
      "Epoch 35 Loss: nan: 100%|██████████| 391/391 [01:48<00:00,  3.62it/s]\n",
      "Epoch 36 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.51it/s]\n",
      "Epoch 37 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.41it/s]\n",
      "Epoch 38 Loss: nan: 100%|██████████| 391/391 [01:58<00:00,  3.31it/s]\n",
      "Epoch 39 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.38it/s]\n",
      "Epoch 40 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.40it/s]\n",
      "Epoch 41 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.41it/s]\n",
      "Epoch 42 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.38it/s]\n",
      "Epoch 43 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.40it/s]\n",
      "Epoch 44 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.58it/s]\n",
      "Epoch 45 Loss: nan: 100%|██████████| 391/391 [01:46<00:00,  3.66it/s]\n",
      "Epoch 46 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.54it/s]\n",
      "Epoch 47 Loss: nan: 100%|██████████| 391/391 [01:47<00:00,  3.63it/s]\n",
      "Epoch 48 Loss: nan: 100%|██████████| 391/391 [01:47<00:00,  3.65it/s]\n",
      "Epoch 49 Loss: nan: 100%|██████████| 391/391 [01:47<00:00,  3.64it/s]\n",
      "Epoch 50 Loss: nan: 100%|██████████| 391/391 [01:47<00:00,  3.65it/s]\n",
      "Epoch 51 Loss: nan: 100%|██████████| 391/391 [01:47<00:00,  3.64it/s]\n",
      "Epoch 52 Loss: nan: 100%|██████████| 391/391 [01:47<00:00,  3.62it/s]\n",
      "Epoch 53 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.56it/s]\n",
      "Epoch 54 Loss: nan: 100%|██████████| 391/391 [01:56<00:00,  3.37it/s]\n",
      "Epoch 55 Loss: nan: 100%|██████████| 391/391 [01:52<00:00,  3.47it/s]\n",
      "Epoch 56 Loss: nan: 100%|██████████| 391/391 [01:53<00:00,  3.45it/s]\n",
      "Epoch 57 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.53it/s]\n",
      "Epoch 58 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.54it/s]\n",
      "Epoch 59 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.57it/s]\n",
      "Epoch 60 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.55it/s]\n",
      "Epoch 61 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.56it/s]\n",
      "Epoch 62 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.55it/s]\n",
      "Epoch 63 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.55it/s]\n",
      "Epoch 64 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.54it/s]\n",
      "Epoch 65 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.57it/s]\n",
      "Epoch 66 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.55it/s]\n",
      "Epoch 67 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.55it/s]\n",
      "Epoch 68 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.54it/s]\n",
      "Epoch 69 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.58it/s]\n",
      "Epoch 70 Loss: nan: 100%|██████████| 391/391 [01:48<00:00,  3.60it/s]\n",
      "Epoch 71 Loss: nan: 100%|██████████| 391/391 [01:57<00:00,  3.34it/s]\n",
      "Epoch 72 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.41it/s]\n",
      "Epoch 73 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.42it/s]\n",
      "Epoch 74 Loss: nan: 100%|██████████| 391/391 [01:56<00:00,  3.37it/s]\n",
      "Epoch 75 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.40it/s]\n",
      "Epoch 76 Loss: nan: 100%|██████████| 391/391 [01:53<00:00,  3.45it/s]\n",
      "Epoch 77 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.38it/s]\n",
      "Epoch 78 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.40it/s]\n",
      "Epoch 79 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.42it/s]\n",
      "Epoch 80 Loss: nan: 100%|██████████| 391/391 [02:03<00:00,  3.18it/s]\n",
      "Epoch 81 Loss: nan: 100%|██████████| 391/391 [02:02<00:00,  3.19it/s]\n",
      "Epoch 82 Loss: nan: 100%|██████████| 391/391 [02:04<00:00,  3.13it/s]\n",
      "Epoch 83 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.37it/s]\n",
      "Epoch 84 Loss: nan: 100%|██████████| 391/391 [01:58<00:00,  3.30it/s]\n",
      "Epoch 85 Loss: nan: 100%|██████████| 391/391 [01:56<00:00,  3.37it/s]\n",
      "Epoch 86 Loss: nan: 100%|██████████| 391/391 [01:56<00:00,  3.35it/s]\n",
      "Epoch 87 Loss: nan: 100%|██████████| 391/391 [01:56<00:00,  3.35it/s]\n",
      "Epoch 88 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.42it/s]\n",
      "Epoch 89 Loss: nan: 100%|██████████| 391/391 [01:58<00:00,  3.30it/s]\n",
      "Epoch 90 Loss: nan: 100%|██████████| 391/391 [01:56<00:00,  3.36it/s]\n",
      "Epoch 91 Loss: nan: 100%|██████████| 391/391 [02:01<00:00,  3.21it/s]\n",
      "Epoch 92 Loss: nan: 100%|██████████| 391/391 [01:59<00:00,  3.26it/s]\n",
      "Epoch 93 Loss: nan: 100%|██████████| 391/391 [02:04<00:00,  3.15it/s]\n",
      "Epoch 94 Loss: nan: 100%|██████████| 391/391 [01:59<00:00,  3.28it/s]\n",
      "Epoch 95 Loss: nan: 100%|██████████| 391/391 [02:00<00:00,  3.23it/s]\n",
      "Epoch 96 Loss: nan: 100%|██████████| 391/391 [02:01<00:00,  3.22it/s]\n",
      "Epoch 97 Loss: nan: 100%|██████████| 391/391 [01:59<00:00,  3.28it/s]\n",
      "Epoch 98 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.39it/s]\n",
      "Epoch 99 Loss: nan: 100%|██████████| 391/391 [02:03<00:00,  3.18it/s]\n",
      "Epoch 100 Loss: nan: 100%|██████████| 391/391 [01:53<00:00,  3.46it/s]\n",
      "Epoch 101 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.39it/s]\n",
      "Epoch 102 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.40it/s]\n",
      "Epoch 103 Loss: nan: 100%|██████████| 391/391 [01:53<00:00,  3.43it/s]\n",
      "Epoch 104 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.38it/s]\n",
      "Epoch 105 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.37it/s]\n",
      "Epoch 106 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.57it/s]\n",
      "Epoch 107 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.55it/s]\n",
      "Epoch 108 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.55it/s]\n",
      "Epoch 109 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.51it/s]\n",
      "Epoch 110 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.50it/s]\n",
      "Epoch 111 Loss: nan: 100%|██████████| 391/391 [01:56<00:00,  3.36it/s]\n",
      "Epoch 112 Loss: nan: 100%|██████████| 391/391 [01:56<00:00,  3.35it/s]\n",
      "Epoch 113 Loss: nan: 100%|██████████| 391/391 [01:53<00:00,  3.45it/s]\n",
      "Epoch 114 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.39it/s]\n",
      "Epoch 115 Loss: nan: 100%|██████████| 391/391 [01:56<00:00,  3.35it/s]\n",
      "Epoch 116 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.37it/s]\n",
      "Epoch 117 Loss: nan: 100%|██████████| 391/391 [01:44<00:00,  3.74it/s]\n",
      "Epoch 118 Loss: nan: 100%|██████████| 391/391 [01:41<00:00,  3.87it/s]\n",
      "Epoch 119 Loss: nan: 100%|██████████| 391/391 [01:39<00:00,  3.95it/s]\n",
      "Epoch 120 Loss: nan: 100%|██████████| 391/391 [01:40<00:00,  3.90it/s]\n",
      "Epoch 121 Loss: nan: 100%|██████████| 391/391 [01:41<00:00,  3.84it/s]\n",
      "Epoch 122 Loss: nan: 100%|██████████| 391/391 [01:56<00:00,  3.37it/s]\n",
      "Epoch 123 Loss: nan: 100%|██████████| 391/391 [01:53<00:00,  3.44it/s]\n",
      "Epoch 124 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.43it/s]\n",
      "Epoch 125 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.42it/s]\n",
      "Epoch 126 Loss: nan: 100%|██████████| 391/391 [02:00<00:00,  3.25it/s]\n",
      "Epoch 127 Loss: nan: 100%|██████████| 391/391 [02:08<00:00,  3.04it/s]\n",
      "Epoch 128 Loss: nan: 100%|██████████| 391/391 [01:59<00:00,  3.27it/s]\n",
      "Epoch 129 Loss: nan: 100%|██████████| 391/391 [01:54<00:00,  3.41it/s]\n",
      "Epoch 130 Loss: nan: 100%|██████████| 391/391 [01:53<00:00,  3.43it/s]\n",
      "Epoch 131 Loss: nan: 100%|██████████| 391/391 [01:53<00:00,  3.44it/s]\n",
      "Epoch 132 Loss: nan: 100%|██████████| 391/391 [01:52<00:00,  3.47it/s]\n",
      "Epoch 133 Loss: nan: 100%|██████████| 391/391 [01:50<00:00,  3.55it/s]\n",
      "Epoch 134 Loss: nan: 100%|██████████| 391/391 [01:51<00:00,  3.50it/s]\n",
      "Epoch 135 Loss: nan: 100%|██████████| 391/391 [01:58<00:00,  3.30it/s]\n",
      "Epoch 136 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.37it/s]\n",
      "Epoch 137 Loss: nan: 100%|██████████| 391/391 [01:55<00:00,  3.37it/s]\n",
      "Epoch 138 Loss: nan: 100%|██████████| 391/391 [01:45<00:00,  3.70it/s]\n",
      "Epoch 139 Loss: nan: 100%|██████████| 391/391 [01:45<00:00,  3.72it/s]\n",
      "Epoch 140 Loss: nan: 100%|██████████| 391/391 [01:52<00:00,  3.47it/s]\n",
      "Epoch 141 Loss: nan: 100%|██████████| 391/391 [01:40<00:00,  3.91it/s]\n",
      "Epoch 142 Loss: nan: 100%|██████████| 391/391 [01:52<00:00,  3.48it/s]\n",
      "Epoch 143 Loss: nan: 100%|██████████| 391/391 [01:48<00:00,  3.61it/s]\n",
      "Epoch 144 Loss: nan: 100%|██████████| 391/391 [01:58<00:00,  3.29it/s]\n",
      "Epoch 145 Loss: nan: 100%|██████████| 391/391 [01:52<00:00,  3.48it/s]\n",
      "Epoch 146 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.57it/s]\n",
      "Epoch 147 Loss: nan: 100%|██████████| 391/391 [01:48<00:00,  3.59it/s]\n",
      "Epoch 148 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.57it/s]\n",
      "Epoch 149 Loss: nan: 100%|██████████| 391/391 [01:49<00:00,  3.56it/s]\n",
      "Epoch 150 Loss: nan: 100%|██████████| 391/391 [01:44<00:00,  3.76it/s]\n"
     ]
    }
   ],
   "source": [
    "#%% Training Loop (Algorithm 1)\n",
    "model = NoPropModel().to(config.device)\n",
    "ema_model = NoPropModel().to(config.device)\n",
    "ema_model.load_state_dict(model.state_dict())\n",
    "ema = EMA()\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, \n",
    "                       weight_decay=config.weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize noise schedule\n",
    "timesteps = torch.arange(config.num_timesteps)\n",
    "alpha_bar = cosine_noise_schedule(timesteps)\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    model.train()\n",
    "    progress_bar = tqdm(trainloader)\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images = images.to(config.device)\n",
    "        labels = labels.to(config.device)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Convert labels to embeddings\n",
    "        u_y = model.embed(labels)\n",
    "        \n",
    "        # Sample random timestep\n",
    "        t = torch.randint(1, config.num_timesteps, (batch_size,)).to(config.device)\n",
    "        alpha_bar_t = alpha_bar[t].view(-1, 1).to(config.device)\n",
    "        \n",
    "        # Forward process (add noise)\n",
    "        epsilon = torch.randn_like(u_y)\n",
    "        z_t = torch.sqrt(alpha_bar_t) * u_y + torch.sqrt(1 - alpha_bar_t) * epsilon\n",
    "        \n",
    "        # Sample random timesteps as indices (shape [batch_size])\n",
    "        t = torch.randint(1, config.num_timesteps, (batch_size,)).to(config.device)\n",
    "\n",
    "        # Forward pass with batched timesteps\n",
    "        u_pred = model(z_t, images, t-1)  # t-1 for 0-based indexing\n",
    "        \n",
    "        # Compute loss (Equation 8)\n",
    "        snr_t = alpha_bar_t / (1 - alpha_bar_t)\n",
    "        snr_t_prev = alpha_bar[t-1].view(-1, 1) / (1 - alpha_bar[t-1].view(-1, 1))\n",
    "        loss_denoise = torch.mean((snr_t - snr_t_prev) * torch.norm(u_pred - u_y, dim=1)**2)\n",
    "        \n",
    "        # Final classification loss\n",
    "        logits = model.final_layer(z_t)\n",
    "        loss_cls = criterion(logits, labels)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = loss_cls + config.eta * loss_denoise\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        ema.update(model, ema_model)\n",
    "        \n",
    "        progress_bar.set_description(f\"Epoch {epoch+1} Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:32<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#%% Evaluation (Fixed Scope Issue)\n",
    "@torch.no_grad()\n",
    "def evaluate(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Initialize noise schedule locally with proper device\n",
    "    timesteps = torch.arange(config.num_timesteps, device=config.device)\n",
    "    alpha_bar = cosine_noise_schedule(timesteps)\n",
    "    \n",
    "    for images, labels in tqdm(testloader):\n",
    "        images = images.to(config.device)\n",
    "        labels = labels.to(config.device)\n",
    "        \n",
    "        # Inference process\n",
    "        z = torch.randn(len(images), config.embed_dim).to(config.device)\n",
    "        \n",
    "        # Iterative denoising\n",
    "        for t_step in reversed(range(config.num_timesteps)):\n",
    "            t_batch = torch.full((len(z),), t_step, \n",
    "                               dtype=torch.long, \n",
    "                               device=config.device)\n",
    "            \n",
    "            u_pred = model(z, images, t_batch)\n",
    "            alpha_bar_t = alpha_bar[t_step]\n",
    "            \n",
    "            # Update rule (keep as tensor operations)\n",
    "            z = (z - (1 - alpha_bar_t) * u_pred) / torch.sqrt(alpha_bar_t)\n",
    "            z += torch.sqrt(1 - alpha_bar_t) * torch.randn_like(z)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = model.final_layer(z)\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100 * correct / total\n",
    "\n",
    "accuracy = evaluate(ema_model, testloader)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Continuous-Time Variant (Algorithm 2)\n",
    "class NoPropCT(nn.Module):\n",
    "    \"\"\"Continuous-time NoProp with neural ODE\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.num_classes, config.embed_dim)\n",
    "        self.block = NoPropBlock(config.embed_dim, config.hidden_dim)\n",
    "        self.time_embed = nn.Embedding(100, config.embed_dim)\n",
    "        self.final_layer = nn.Linear(config.embed_dim, config.num_classes)\n",
    "        \n",
    "    def forward(self, z, x, t):\n",
    "        t_embed = self.time_embed(t)\n",
    "        return self.block(z + t_embed, x)\n",
    "\n",
    "# Training loop for continuous-time would involve:\n",
    "# - Random time sampling\n",
    "# - Neural ODE solver integration\n",
    "# - Different noise schedule handling\n",
    "# (Implementation similar to discrete-time but with continuous components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Flow Matching Variant (Algorithm 3)\n",
    "class NoPropFM(nn.Module):\n",
    "    \"\"\"Flow matching variant\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.num_classes, config.embed_dim)\n",
    "        self.block = NoPropBlock(config.embed_dim, config.hidden_dim)\n",
    "        self.time_embed = nn.Embedding(100, config.embed_dim)\n",
    "        self.final_layer = nn.Linear(config.embed_dim, config.num_classes)\n",
    "        \n",
    "    def forward(self, z, x, t):\n",
    "        t_embed = self.time_embed(t)\n",
    "        return self.block(z + t_embed, x)\n",
    "\n",
    "# Training loop for flow matching would involve:\n",
    "# - Linear interpolation between noise and target\n",
    "# - Vector field prediction\n",
    "# - Anchor loss implementation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
