{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c5b1273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "#%% -------- 1. Configuration & Imports --------\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import splitfolders\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torchmetrics\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Configuration\n",
    "NUM_CLASSES = 4\n",
    "IMG_SIZE = 384\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 4\n",
    "PRECISION = '16-mixed'\n",
    "EPOCHS = 150\n",
    "LR = 1e-4\n",
    "BATCH_SIZE = 32\n",
    "DATA_ROOT = \"C:\\\\Users\\\\DELL 5540\\\\Desktop\\\\Brachial\\\\2nd Classification\"\n",
    "SPLIT_ROOT = \"C:\\\\Users\\\\DELL 5540\\\\Desktop\\\\Brachial\\\\Split_Dataset\"\n",
    "SEED = 42\n",
    "CLASS_NAMES = ['Type 0', 'Type 1 Neurapraxia', 'Type 2 Axonotemsis', 'Type 3 Neurotmesis']\n",
    "\n",
    "# Reproducibility\n",
    "pl.seed_everything(SEED)\n",
    "torch.use_deterministic_algorithms(False)  # Add this line\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa2479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 2. Enhanced Data Preparation Cell --------\n",
    "def prepare_dataset():\n",
    "    splitfolders.ratio(\n",
    "        DATA_ROOT,\n",
    "        output=SPLIT_ROOT,\n",
    "        seed=SEED,\n",
    "        ratio=(0.7, 0.15, 0.15),\n",
    "        group_prefix=None,\n",
    "        move=False\n",
    "    )\n",
    "\n",
    "if not os.path.exists(os.path.join(SPLIT_ROOT, 'train')):\n",
    "    prepare_dataset()\n",
    "\n",
    "# Enhanced MRI-specific transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)  # Fixed normalization for 3 channels\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = ImageFolder(os.path.join(SPLIT_ROOT, 'train'), train_transform)\n",
    "val_dataset = ImageFolder(os.path.join(SPLIT_ROOT, 'val'), val_transform)\n",
    "test_dataset = ImageFolder(os.path.join(SPLIT_ROOT, 'test'), val_transform)\n",
    "\n",
    "# Class balancing with pos_weight\n",
    "class_counts = np.bincount(train_dataset.targets)\n",
    "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float32)  # Add this line\n",
    "pos_weight = torch.tensor([class_counts[0]/class_counts[1]], dtype=torch.float32)\n",
    "samples_weights = class_weights[train_dataset.targets]\n",
    "sampler = WeightedRandomSampler(samples_weights, len(samples_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435fa390",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 2. Medical Data Pipeline --------\n",
    "class MedicalAugmentation:\n",
    "    \"\"\"MRI-optimized augmentation pipeline\"\"\"\n",
    "    def __init__(self):\n",
    "        self.affine = transforms.RandomAffine(\n",
    "            degrees=(-7, 7), \n",
    "            translate=(0.05, 0.05),\n",
    "            scale=(0.9, 1.1)\n",
    "        )\n",
    "        self.elastic = transforms.ElasticTransform(\n",
    "            alpha=50.0,\n",
    "            sigma=5.0\n",
    "        )\n",
    "        self.color_jitter = transforms.ColorJitter(\n",
    "            brightness=0.1,\n",
    "            contrast=0.2,\n",
    "            saturation=0.1\n",
    "        )\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        img = self.affine(img)\n",
    "        if np.random.rand() > 0.7:\n",
    "            img = self.elastic(img)\n",
    "        img = self.color_jitter(img)\n",
    "        return img\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    MedicalAugmentation(),\n",
    "    transforms.RandomErasing(p=0.4, scale=(0.02, 0.15)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset preparation\n",
    "if not os.path.exists(os.path.join(SPLIT_ROOT, 'train')):\n",
    "    splitfolders.ratio(\n",
    "        DATA_ROOT,\n",
    "        output=SPLIT_ROOT,\n",
    "        seed=SEED,\n",
    "        ratio=(0.8, 0.1, 0.1),\n",
    "        group_prefix=None,\n",
    "        move=False,\n",
    "        oversample='auto'\n",
    "    )\n",
    "\n",
    "train_dataset = ImageFolder(os.path.join(SPLIT_ROOT, 'train'), train_transform)\n",
    "val_dataset = ImageFolder(os.path.join(SPLIT_ROOT, 'val'), val_transform)\n",
    "test_dataset = ImageFolder(os.path.join(SPLIT_ROOT, 'test'), val_transform)\n",
    "\n",
    "# Class balancing\n",
    "class_counts = np.bincount(train_dataset.targets)\n",
    "class_weights = torch.tensor([len(train_dataset) / (count * NUM_CLASSES) for count in class_counts], dtype=torch.float32)\n",
    "sampler = WeightedRandomSampler(weights=class_weights[train_dataset.targets], num_samples=len(train_dataset), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb8415a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 4. Enhanced Model Definition Cell --------\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        loss = (1-pt)**self.gamma * ce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "class BrachialPlexusResMT(nn.Module):\n",
    "    def __init__(self, num_classes=4, img_size=224, patch_size=32, embed_dim=512, \n",
    "                 num_heads=8, num_layers=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ResNet-50 Backbone\n",
    "        self.cnn_backbone = models.resnet50(pretrained=True)\n",
    "        self.cnn_backbone = nn.Sequential(*list(self.cnn_backbone.children())[:-2])\n",
    "        \n",
    "        # Freeze initial layers\n",
    "        for param in self.cnn_backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Unfreeze last residual block\n",
    "        for param in self.cnn_backbone[-1].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Feature adaptation\n",
    "        self.feature_adapt = nn.Sequential(\n",
    "            nn.Conv2d(2048, embed_dim, 1),\n",
    "            nn.BatchNorm2d(embed_dim),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(49, 1, embed_dim))\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layers = TransformerEncoderLayer(\n",
    "            embed_dim, num_heads, dim_feedforward=2048, dropout=dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(embed_dim, num_classes)  # Changed to 4 outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_backbone(x)\n",
    "        x = self.feature_adapt(x)\n",
    "        x = x.flatten(2).permute(2, 0, 1)\n",
    "        x = x + self.positional_encoding\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75f06afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "C:\\Users\\DELL 5540\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\plugins\\precision\\amp.py:54: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#%% -------- 5. Enhanced Training Setup Cell --------\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, total_steps):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = BrachialPlexusResMT(num_classes=NUM_CLASSES)\n",
    "        \n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.criterion = FocalLoss(alpha=class_weights.to(device))\n",
    "\n",
    "        # Metrics\n",
    "        self.train_acc = torchmetrics.Accuracy(task='multiclass', num_classes=NUM_CLASSES)\n",
    "        self.train_auc = torchmetrics.AUROC(task='multiclass', num_classes=NUM_CLASSES)\n",
    "        self.val_acc = torchmetrics.Accuracy(task='multiclass', num_classes=NUM_CLASSES)\n",
    "        self.val_auc = torchmetrics.AUROC(task='multiclass', num_classes=NUM_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc(preds, y)\n",
    "        self.train_auc(logits.softmax(dim=1), y)\n",
    "        \n",
    "        self.log_dict({'train_loss': loss, 'train_acc': self.train_acc, 'train_auc': self.train_auc},\n",
    "                     prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.criterion(logits, y)\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_acc(preds, y)\n",
    "        self.val_auc(logits.softmax(dim=1), y)\n",
    "        \n",
    "        self.log_dict({'val_loss': loss, 'val_acc': self.val_acc, 'val_auc': self.val_auc},\n",
    "                     prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Differential learning rates\n",
    "        backbone_params = []\n",
    "        new_params = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'cnn_backbone.layer4' in name:\n",
    "                backbone_params.append(param)\n",
    "            else:\n",
    "                new_params.append(param)\n",
    "        \n",
    "        optimizer = optim.AdamW(\n",
    "            [\n",
    "                {'params': backbone_params, 'lr': LR/10},\n",
    "                {'params': new_params, 'lr': LR}\n",
    "            ],\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=[LR/10, LR],\n",
    "            total_steps=self.hparams.total_steps\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "# Initialize\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, sampler=sampler, \n",
    "                         num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, BATCH_SIZE, num_workers=NUM_WORKERS,\n",
    "                       pin_memory=True)\n",
    "model = LitModel(total_steps=EPOCHS*len(train_loader))\n",
    "\n",
    "# Callbacks\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_auc',\n",
    "    mode='max',\n",
    "    filename='best-{epoch}-{val_auc:.3f}',\n",
    "    save_top_k=2\n",
    ")\n",
    "\n",
    "early_stop = pl.callbacks.EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    patience=25,  # Increased patience\n",
    "    mode='max',\n",
    "    min_delta=0.001\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=1,\n",
    "    max_epochs=EPOCHS,\n",
    "    precision=PRECISION,\n",
    "    callbacks=[checkpoint, early_stop],\n",
    "    accumulate_grad_batches=4,\n",
    "    gradient_clip_val=1.0,\n",
    "    log_every_n_steps=10,\n",
    "    deterministic=\"warn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73f0c7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                | Params\n",
      "--------------------------------------------------\n",
      "0 | model     | BrachialPlexusResMT | 34.0 M\n",
      "1 | criterion | FocalLoss           | 0     \n",
      "2 | train_acc | MulticlassAccuracy  | 0     \n",
      "3 | train_auc | MulticlassAUROC     | 0     \n",
      "4 | val_acc   | MulticlassAccuracy  | 0     \n",
      "5 | val_auc   | MulticlassAUROC     | 0     \n",
      "--------------------------------------------------\n",
      "25.5 M    Trainable params\n",
      "8.5 M     Non-trainable params\n",
      "34.0 M    Total params\n",
      "136.174   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c262825d738d45279f26158a41a7fdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL 5540\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (144) must match the size of tensor b (49) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#%% -------- 6. Training Execution Cell --------\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m best_model \u001b[38;5;241m=\u001b[39m LitModel\u001b[38;5;241m.\u001b[39mload_from_checkpoint(trainer\u001b[38;5;241m.\u001b[39mcheckpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:545\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 545\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:581\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    575\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[0;32m    577\u001b[0m     ckpt_path,\n\u001b[0;32m    578\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    579\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    580\u001b[0m )\n\u001b[1;32m--> 581\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:990\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 990\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    995\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1034\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m   1033\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[1;32m-> 1034\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m   1036\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1063\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1060\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[1;32m-> 1063\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1065\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\loops\\utilities.py:181\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m, in \u001b[0;36mLitModel.validation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m     34\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 35\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(logits, y)\n\u001b[0;32m     38\u001b[0m     preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m, in \u001b[0;36mLitModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[7], line 57\u001b[0m, in \u001b[0;36mBrachialPlexusResMT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_adapt(x)\n\u001b[0;32m     56\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_encoding\u001b[49m\n\u001b[0;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(x)\n\u001b[0;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (144) must match the size of tensor b (49) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "#%% -------- 6. Training Execution Cell --------\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "best_model = LitModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f32eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 6. Advanced Evaluation --------\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "\n",
    "\n",
    "class MedicalEvaluator:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.tta_transforms = [\n",
    "            lambda x: x,\n",
    "            lambda x: torch.flip(x, [-1]),\n",
    "            lambda x: torch.rot90(x, 1, [-2, -1]),\n",
    "            lambda x: torch.rot90(x, -1, [-2, -1])\n",
    "        ]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad(), autocast():\n",
    "            logits = torch.zeros((x.size(0), NUM_CLASSES), device=x.device)\n",
    "            for transform in self.tta_transforms:\n",
    "                augmented = transform(x)\n",
    "                logits += self.model(augmented)\n",
    "            return logits / len(self.tta_transforms)\n",
    "    \n",
    "    def evaluate(self, loader):\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        all_probs = []\n",
    "        \n",
    "        for x, y in loader:\n",
    "            x = x.to(self.model.device)\n",
    "            logits = self.predict(x)\n",
    "            \n",
    "            probs = logits.softmax(1)\n",
    "            preds = probs.argmax(1)\n",
    "            \n",
    "            all_probs.append(probs.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(y)\n",
    "            \n",
    "        return (\n",
    "            torch.cat(all_preds),\n",
    "            torch.cat(all_targets),\n",
    "            torch.cat(all_probs)\n",
    "        )\n",
    "\n",
    "# Load best model\n",
    "best_model = LitMedicalModel.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path\n",
    ")\n",
    "evaluator = MedicalEvaluator(best_model)\n",
    "\n",
    "# Test evaluation\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "preds, targets, probs = evaluator.evaluate(test_loader)\n",
    "\n",
    "# Classification report\n",
    "print(\"Medical Diagnostic Report:\")\n",
    "print(classification_report(\n",
    "    targets.numpy(),\n",
    "    preds.numpy(),\n",
    "    target_names=CLASS_NAMES,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(targets.numpy(), preds.numpy())\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.title('Medical Diagnostic Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC curves\n",
    "fpr, tpr, _ = roc_curve(targets.numpy(), probs.numpy()[:, 1], pos_label=1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72d45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% -------- 7. Visualization Utilities --------\n",
    "def visualize_attention(model, img_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model.model.cnn(img_tensor.unsqueeze(0))\n",
    "        features = model.model.adapt(features)\n",
    "        B, C, H, W = features.shape\n",
    "        spatial = features.view(B, C, -1).permute(0, 2, 1)\n",
    "        spatial += model.model.pos_embed\n",
    "        attn_weights = model.model.attention(spatial).squeeze()\n",
    "        \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(img_tensor.permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(attn_weights.view(H, W).cpu().numpy(), cmap='jet')\n",
    "    plt.title(\"Attention Map\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "sample, _ = test_dataset[0]\n",
    "visualize_attention(best_model, sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
